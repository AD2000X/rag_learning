{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0cba5b9c34b2464b89ef3a3067a00bd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3d695a9103fd43d7acc5a3d881e10082",
              "IPY_MODEL_60fd1dad2a444c4aac4a38ddfdf057f5",
              "IPY_MODEL_68cd133f95f546bf9cfc6990ca30cfb0"
            ],
            "layout": "IPY_MODEL_da067304b6bd4aa3b84c6d6042527768"
          }
        },
        "3d695a9103fd43d7acc5a3d881e10082": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac0fe294dfda46e9977ab5d3d3e03ff7",
            "placeholder": "​",
            "style": "IPY_MODEL_20fdaaedbfd0469a848e2cb318bd0c58",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "60fd1dad2a444c4aac4a38ddfdf057f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32012e8092c34f3abee55a1179278022",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8e8c0f0439074c429c102f196cd8de0f",
            "value": 2
          }
        },
        "68cd133f95f546bf9cfc6990ca30cfb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83c8eafc073c498d93135aeedf4fdd68",
            "placeholder": "​",
            "style": "IPY_MODEL_0461b91a087e4ebd9d86da638bad1951",
            "value": " 2/2 [00:09&lt;00:00,  4.03s/it]"
          }
        },
        "da067304b6bd4aa3b84c6d6042527768": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac0fe294dfda46e9977ab5d3d3e03ff7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20fdaaedbfd0469a848e2cb318bd0c58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32012e8092c34f3abee55a1179278022": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e8c0f0439074c429c102f196cd8de0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "83c8eafc073c498d93135aeedf4fdd68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0461b91a087e4ebd9d86da638bad1951": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setting"
      ],
      "metadata": {
        "id": "qMKl3jd-LLyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# os.environ[ 'HF_HOME' ]  = ' /run/cache/ '\n",
        "\n",
        "\n",
        "'''\n",
        "HF_HOME Environment Variable: In Hugging Face’s transformers library,\n",
        "the HF_HOME variable is used to specify the directory where the library downloads and stores model files, configurations, and other assets.\n",
        "By default, this is usually in a folder like ~/.cache/huggingface on most systems.\n",
        "\n",
        "Setting HF_HOME: By defining HF_HOME as /run/cache/,\n",
        "you are directing Hugging Face to store all these files in /run/cache/ instead of the default location.\n",
        "This can be useful if you want to:\n",
        "\n",
        "Control where these cached files go.\n",
        "Use a faster storage option (e.g., RAM disk).\n",
        "Save space in your main storage directory.\n",
        "\n",
        "Why This Matters in Colab: In a Colab session, if you set HF_HOME to a directory like /run/cache/,\n",
        "you might benefit from faster read/write operations due to Colab’s ephemeral memory.\n",
        "However, remember that Colab’s runtime is temporary,\n",
        "so if you want to persist the models between sessions,\n",
        "consider mounting Google Drive and setting HF_HOME to a directory there.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "RQ2d2bHOmqLP",
        "outputId": "63678225-c272-452a-d834-73777b22fe8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nHF_HOME Environment Variable: In Hugging Face’s transformers library, \\nthe HF_HOME variable is used to specify the directory where the library downloads and stores model files, configurations, and other assets. \\nBy default, this is usually in a folder like ~/.cache/huggingface on most systems.\\n\\nSetting HF_HOME: By defining HF_HOME as /run/cache/, \\nyou are directing Hugging Face to store all these files in /run/cache/ instead of the default location. \\nThis can be useful if you want to:\\n\\nControl where these cached files go.\\nUse a faster storage option (e.g., RAM disk).\\nSave space in your main storage directory.\\n\\nWhy This Matters in Colab: In a Colab session, if you set HF_HOME to a directory like /run/cache/, \\nyou might benefit from faster read/write operations due to Colab’s ephemeral memory. \\nHowever, remember that Colab’s runtime is temporary, \\nso if you want to persist the models between sessions, \\nconsider mounting Google Drive and setting HF_HOME to a directory there.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# export HF_HOME= '/run/cache/'\n",
        "# jupyter notebook\n",
        "\n",
        "'''\n",
        "export HF_HOME='/run/cache/':\n",
        "\n",
        "This command is used in a Unix-based shell (like Bash) to set the environment variable HF_HOME to the path /run/cache/.\n",
        "Unlike the Python os.environ assignment, which only affects the environment for the current Python session,\n",
        "export sets the variable globally for any processes started in that terminal session (e.g., other applications or scripts).\n",
        "The HF_HOME environment variable would direct Hugging Face libraries to use /run/cache/ for caching models,\n",
        "similar to the Python setup, but at a global shell level.\n",
        "\n",
        "jupyter notebook:\n",
        "\n",
        "This command starts a Jupyter Notebook server. With HF_HOME set beforehand,\n",
        "any notebooks launched would inherit this environment variable.\n",
        "This means that if you use Hugging Face’s transformers library in Jupyter,\n",
        "it would also cache its files in /run/cache/,\n",
        "as specified by the HF_HOME variable set in the shell.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "cLmKWIxNpyC7",
        "outputId": "bb5027c5-735e-49f3-c5f9-cbbaa580ce93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nexport HF_HOME='/run/cache/':\\n\\nThis command is used in a Unix-based shell (like Bash) to set the environment variable HF_HOME to the path /run/cache/.\\nUnlike the Python os.environ assignment, which only affects the environment for the current Python session, \\nexport sets the variable globally for any processes started in that terminal session (e.g., other applications or scripts).\\nThe HF_HOME environment variable would direct Hugging Face libraries to use /run/cache/ for caching models, \\nsimilar to the Python setup, but at a global shell level.\\n\\njupyter notebook:\\n\\nThis command starts a Jupyter Notebook server. With HF_HOME set beforehand, \\nany notebooks launched would inherit this environment variable.\\nThis means that if you use Hugging Face’s transformers library in Jupyter, \\nit would also cache its files in /run/cache/, \\nas specified by the HF_HOME variable set in the shell.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Difference Between Shell and Python Environment Variables\n",
        "Python Environment (os.environ): Only affects the current Python runtime environment. Once you exit Python or switch to another environment, the variable will not persist.\n",
        "Shell Environment (export): Persists for the duration of the terminal session, affecting all child processes or applications (e.g., Jupyter Notebook) that are started in that session.\n",
        "In Colab, the Python os.environ approach is usually sufficient, but if you're running commands outside of Colab (e.g., on a local system), export HF_HOME=... would ensure that any application run in that terminal would recognize the HF_HOME setting."
      ],
      "metadata": {
        "id": "fQ5EPvEQqASe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Mount Google Drive\n",
        "mount Google Drive in Colab and set HF_HOME to a directory within Google Drive so that Hugging Face models and data persist across sessions:"
      ],
      "metadata": {
        "id": "zeKt8lxincHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# After running this, you’ll be prompted to authorize access to your Google Drive. Once authorized, your Google Drive will be accessible at /content/drive."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRC8O00QnDww",
        "outputId": "13d2480a-0155-4e9d-fe8b-369ed6a76c48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Set HF_HOME to a Directory in Google Drive\n",
        "Choose or create a folder within Google Drive to store Hugging Face models. Here’s how to set it up:"
      ],
      "metadata": {
        "id": "oCOUmvPmngpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the path in Google Drive\n",
        "hf_cache_dir = '/content/drive/MyDrive/huggingface_cache'\n",
        "\n",
        "# Create the directory if it doesn't already exist\n",
        "os.makedirs(hf_cache_dir, exist_ok=True)\n",
        "\n",
        "# Set the HF_HOME environment variable to the chosen path\n",
        "os.environ['HF_HOME'] = hf_cache_dir\n"
      ],
      "metadata": {
        "id": "tRoqM8ZMn3Cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Explanation\n",
        "hf_cache_dir: This variable holds the path to a directory in Google Drive where Hugging Face files will be saved.\n",
        "Here, it’s set to '/content/drive/MyDrive/huggingface_cache', but you can change this path to any folder within your Google Drive.\n",
        "os.makedirs(): This ensures the directory exists; it will create it if it doesn’t.\n",
        "os.environ['HF_HOME']: Sets the environment variable so that any downloads or cached files from Hugging Face will go to this directory.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "J-IAuDbon-qq",
        "outputId": "91d51d0c-5bde-4149-de88-b8f0b3b39c77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nExplanation\\nhf_cache_dir: This variable holds the path to a directory in Google Drive where Hugging Face files will be saved. \\nHere, it’s set to '/content/drive/MyDrive/huggingface_cache', but you can change this path to any folder within your Google Drive.\\nos.makedirs(): This ensures the directory exists; it will create it if it doesn’t.\\nos.environ['HF_HOME']: Sets the environment variable so that any downloads or cached files from Hugging Face will go to this directory.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Verification\n",
        "After running the above setup, whenever you download a Hugging Face model, it will be saved in the specified Google Drive folder. You can check this by listing files in that folder:"
      ],
      "metadata": {
        "id": "21-5he-5oFZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List files in the cache directory to confirm downloads\n",
        "os.listdir(hf_cache_dir)\n",
        "\n",
        "# This setup ensures that any models downloaded during your Colab session will be available in Google Drive, making them persistent across different sessions."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJi-4bDdoIjf",
        "outputId": "886358dc-759e-4215-8379-b71a01c871c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hub']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Open source LLM"
      ],
      "metadata": {
        "id": "wjCMamlFoMxc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAID5GCefvUC",
        "outputId": "8c5c8064-5fa7-42ee-84c4-3279744fb972"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-06 14:01:41--  https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3832 (3.7K) [text/plain]\n",
            "Saving to: ‘minsearch.py’\n",
            "\n",
            "\rminsearch.py          0%[                    ]       0  --.-KB/s               \rminsearch.py        100%[===================>]   3.74K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-11-06 14:01:41 (85.2 MB/s) - ‘minsearch.py’ saved [3832/3832]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!rm -f minsearch.py\n",
        "!wget https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers accelerate bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCTbak9igFiG",
        "outputId": "4e6f3765-2ac0-4a77-e66c-1ca9e586de7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.44.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.0+cu121)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import minsearch\n",
        "\n",
        "docs_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1'\n",
        "docs_response = requests.get(docs_url)\n",
        "documents_raw = docs_response.json()\n",
        "\n",
        "documents = []\n",
        "\n",
        "for course in documents_raw:\n",
        "    course_name = course['course']\n",
        "\n",
        "    for doc in course['documents']:\n",
        "        doc['course'] = course_name\n",
        "        documents.append(doc)\n",
        "\n",
        "index = minsearch.Index(\n",
        "    text_fields=[\"question\", \"text\", \"section\"],\n",
        "    keyword_fields=[\"course\"]\n",
        ")\n",
        "\n",
        "index.fit(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDCPl8AMhAQ0",
        "outputId": "c462c4e6-86fa-4b14-fe0f-161a64da1d88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<minsearch.Index at 0x7bcd0c324310>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def search(query):\n",
        "    boost = {'question': 3.0, 'section': 0.5}\n",
        "\n",
        "    results = index.search(\n",
        "        query=query,\n",
        "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
        "        boost_dict=boost,\n",
        "        num_results=5\n",
        "    )\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "CKRVaVVWhBuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_prompt(query, search_results):\n",
        "    prompt_template = \"\"\"\n",
        "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
        "Use only the facts from the CONTEXT when answering the QUESTION.\n",
        "\n",
        "QUESTION: {question}\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\"\"\".strip()\n",
        "\n",
        "    context = \"\"\n",
        "\n",
        "    for doc in search_results:\n",
        "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
        "\n",
        "    prompt = prompt_template.format(question=query, context=context).strip()\n",
        "    return prompt\n",
        "\n",
        "def llm(prompt):\n",
        "    response = client.chat.completions.create(\n",
        "        model='gpt-4o',\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "Fs4jN_5_hDO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rag(query):\n",
        "    search_results = search(query)\n",
        "    prompt = build_prompt(query, search_results)\n",
        "    answer = llm(prompt)\n",
        "    return answer"
      ],
      "metadata": {
        "id": "Nv9ny_hLhEnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###\n",
        "The command !df -h in Colab shows the disk usage and storage distribution across various filesystems in a human-readable format (-h)."
      ],
      "metadata": {
        "id": "cjUjekfSzGWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!df -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5BHbEMdhdAF",
        "outputId": "aeb0aa89-ca21-4fdf-aa43-996fd995bc9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         236G   55G  182G  23% /\n",
            "tmpfs            64M     0   64M   0% /dev\n",
            "shm              26G  4.0K   26G   1% /dev/shm\n",
            "/dev/root       2.0G  1.2G  820M  59% /usr/sbin/docker-init\n",
            "tmpfs            27G  1.6M   27G   1% /var/colab\n",
            "/dev/nvme0n1p1  242G   71G  172G  30% /kaggle/input\n",
            "tmpfs            27G     0   27G   0% /proc/acpi\n",
            "tmpfs            27G     0   27G   0% /proc/scsi\n",
            "tmpfs            27G     0   27G   0% /sys/firmware\n",
            "drive           100G   47G   54G  47% /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filesystem: This column lists the specific disk partitions or types of storage resources available in the Colab environment.\n",
        "\n",
        "Size: The total capacity of each filesystem or disk partition.\n",
        "\n",
        "Used: The amount of storage currently in use.\n",
        "\n",
        "Avail: The available storage left on each filesystem.\n",
        "\n",
        "Use%: The percentage of the filesystem's total space that is in use.\n",
        "\n",
        "Mounted on: The directory where each filesystem is \"mounted,\" meaning where in the directory structure it can be accessed.\n",
        "\n",
        "Explanation of Each Line\n",
        "overlay: The main filesystem that Colab allocates for your runtime environment, with 236 GB total, 193 GB free (19% in use). This is where most files you create or save during your session are stored.\n",
        "/dev/shm: Shared memory for inter-process communication, with a large allocation (26 GB) but almost entirely free.\n",
        "/dev/root: A smaller system partition with 2.0 GB total and 820 MB free, where essential system binaries and services might be stored. This is almost entirely managed by Colab.\n",
        "/var/colab: A small, temporary filesystem, often used by Colab internally.\n",
        "/dev/nvme0n1p1: A faster NVMe drive where data files, particularly those accessed during computations, may be cached. This drive has 242 GB total, with 175 GB available.\n",
        "tmpfs entries (/proc/acpi, /proc/scsi, /sys/firmware): Temporary file systems generally used for reading device and system information, often showing no data usage as they’re mostly read-only.\n",
        "In a Colab environment, these filesystems serve specific purposes, mainly to support system functions and provide temporary storage for runtime operations. Note that Colab storage is typically ephemeral, so data not saved explicitly (e.g., to Google Drive) may be lost after the session ends."
      ],
      "metadata": {
        "id": "VCJKkiZ_zVkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install accelerate\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration"
      ],
      "metadata": {
        "id": "ZtZ1q7TMhrv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\", device_map=\"auto\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191,
          "referenced_widgets": [
            "0cba5b9c34b2464b89ef3a3067a00bd8",
            "3d695a9103fd43d7acc5a3d881e10082",
            "60fd1dad2a444c4aac4a38ddfdf057f5",
            "68cd133f95f546bf9cfc6990ca30cfb0",
            "da067304b6bd4aa3b84c6d6042527768",
            "ac0fe294dfda46e9977ab5d3d3e03ff7",
            "20fdaaedbfd0469a848e2cb318bd0c58",
            "32012e8092c34f3abee55a1179278022",
            "8e8c0f0439074c429c102f196cd8de0f",
            "83c8eafc073c498d93135aeedf4fdd68",
            "0461b91a087e4ebd9d86da638bad1951"
          ]
        },
        "id": "PnccCmw5hsg5",
        "outputId": "651c95da-bde1-48f5-c443-0ef992b70f96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0cba5b9c34b2464b89ef3a3067a00bd8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"translate English to German: How old are you?\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "input_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxD8qSbqhbL-",
        "outputId": "466ec945-b62a-4726-8ad1-6c47b99f12a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[13959,  1566,    12,  2968,    10,   571,   625,    33,    25,    58,\n",
              "             1]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The reason the output tensor is 11-dimensional instead of 10 is due to the special tokens added by the tokenizer.\n",
        "Most tokenizers, especially for models like BERT, T5, or GPT, add a special \"end-of-sequence\" token (like [EOS] or <|endoftext|>) to indicate the end of the input. In this case, it seems a special token, most likely an end-of-sequence token (<eos>), was appended at the end of the tokenized sequence.\n",
        "\n",
        "Here’s the breakdown:\n",
        "\n",
        "Input Sequence: \"translate English to German: How old are you?\"\n",
        "Tokenization: The tokenizer splits this input into 10 tokens, one for each word or punctuation.\n",
        "Special Token Addition: The tokenizer appends the <eos> token (or equivalent) to the sequence, making the total length 11 tokens."
      ],
      "metadata": {
        "id": "hr_YwqcUxLxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why Add a Special Token?\n",
        "The special <eos> token helps the model understand where the input ends, which is especially useful for tasks involving sequences like translation, where you might be working with different lengths and boundaries in input and output text."
      ],
      "metadata": {
        "id": "xc7dHPV-xkDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Necessary imports\n",
        "# from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# # Initialize the tokenizer and model\n",
        "# tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
        "# model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\", device_map=\"auto\")\n",
        "\n",
        "# # Tokenize without special tokens during the tokenization process\n",
        "# input_text = \"translate English to German: How old are you?\"\n",
        "# input_ids = tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(\"cuda\")\n"
      ],
      "metadata": {
        "id": "28ypCGFvwCyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input_ids"
      ],
      "metadata": {
        "id": "XwaYTn0LxDWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(input_ids)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLH2L5WqvA4b",
        "outputId": "065c7a6c-4ac3-4241-c309-5c34aac99489"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pad> Wie alt sind Sie?</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs\n",
        "\n",
        "# <pad> Wie alt sind Sie?</s>\n",
        "# tensor([[   0, 2739, 4445,  436,  292,   58,    1]], device='cuda:0')\n",
        "# 0 = <pad>, ..., 1 = </s>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvnETCrnyQqf",
        "outputId": "933ea660-d989-4704-a1b9-6a88f7e28b20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   0, 2739, 4445,  436,  292,   58,    1]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Put everything together"
      ],
      "metadata": {
        "id": "nUDJ3uAfy0qu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"translate English to German: How old are you?\"\n",
        "\n",
        "# move below lines into def llm(prompt):\n",
        "\n",
        "# input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "# outputs = model.generate(input_ids)\n",
        "# result = tokenizer.decode(outputs[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVijIHijy3Zf",
        "outputId": "802e56c5-4830-4aef-f8d5-46b55e6f59cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "v-Bcnz3gznse",
        "outputId": "d93b34ff-8092-4794-8d11-827414b8fab6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<pad> Wie alt sind Sie?</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### parameters\n",
        "top_k=50:\n",
        "\n",
        "Top-K Sampling: At each step of text generation, the model considers only the top K highest-scoring options (here, 50 options), ignoring the rest. This effectively reduces the likelihood of selecting low-probability, less logical words. Increasing the K value can make the generated text more creative but may also introduce more variability.\n",
        "\n",
        "top_p=0.95:\n",
        "\n",
        "Top-P (Nucleus Sampling): Similar to top_k, but selects options based on cumulative probability. At each generation step, the model selects from a subset of tokens whose cumulative probability reaches p (here, 0.95). Using top_p allows the generated text to be more flexible without fixing the K value, suitable for tasks that require a balance between creativity and coherence.\n",
        "\n",
        "do_sample=False:\n",
        "\n",
        "Enable Random Sampling: If set to True, the model will randomly choose words within the limits of top_k or top_p. Setting it to False means the model will always choose the highest-scoring option, suitable for more consistent and rigorous outputs. When do_sample=False, the generated text tends to be more consistent but less creative.\n",
        "\n",
        "num_beams=4:\n",
        "\n",
        "Number of Beams in Beam Search: Sets the number of beams for beam search, which is a search strategy that considers multiple potential paths during generation and then selects the one with the highest score. Here, it is set to use 4 beams. Beam search can improve the quality of the generated text and is usually applied to tasks requiring high accuracy, such as translation or summarization.\n",
        "\n",
        "no_repeat_ngram_size=3:\n",
        "\n",
        "Avoid Repeating N-grams: Sets the size of n (here, 3), so the model avoids generating the same n-gram (such as three consecutive words) repeatedly in the text. This parameter is helpful for reducing overly repetitive content.\n",
        "\n",
        "early_stopping=True:\n",
        "\n",
        "Early Stopping: During beam search, if all beams have generated an EOS (end-of-sequence) token, generation stops early, avoiding overly long text. Enabling this option helps produce text of a suitable length, especially when overly long output is not needed."
      ],
      "metadata": {
        "id": "75vN0SiZ_X-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_prompt(query, search_results):\n",
        "    prompt_template = \"\"\"\n",
        "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
        "Use only the facts from the CONTEXT when answering the QUESTION.\n",
        "\n",
        "QUESTION: {question}\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\"\"\".strip()\n",
        "\n",
        "    context = \"\"\n",
        "\n",
        "    for doc in search_results:\n",
        "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
        "\n",
        "    prompt = prompt_template.format(question=query, context=context).strip()\n",
        "    return prompt\n",
        "\n",
        "def llm(prompt, generate_params=None):\n",
        "    if generate_params is None:\n",
        "        generate_params = {}\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_length=generate_params.get(\"max_length\", 100),\n",
        "        num_beams=generate_params.get(\"num_beams\", 5),\n",
        "        do_sample=generate_params.get(\"do_sample\", False),\n",
        "        temperature=generate_params.get(\"temperature\", 1.0),\n",
        "        top_k=generate_params.get(\"top_k\", 50),\n",
        "        top_p=generate_params.get(\"top_p\", 0.95),\n",
        "    )\n",
        "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return result"
      ],
      "metadata": {
        "id": "uWe2dvAIzwoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# outputs = model.generate(\n",
        "#     input_ids,\n",
        "#     max_new_tokens=100,\n",
        "#     num_beams=4,\n",
        "#     no_repeat_ngram_size=3,\n",
        "#     early_stopping=True,\n",
        "#     do_sample=False,  # Sampling is not used\n",
        "#     temperature=1.0   # This can be removed, as it is not used when sampling is disabled\n",
        "# )\n",
        "\n",
        "#  Longer response:\n",
        "# Use max_length\n",
        "# outputs = model.generate(input_ids,\n",
        "#                          max_length=512)  # Set the total tokens limit, including input tokens\n",
        "\n",
        "# Or use max_new_tokens\n",
        "# outputs = model.generate(input_ids,\n",
        "#                          max_new_tokens=256)  # Set the number of new tokens to generate, excluding input tokens\n",
        "\n",
        "# max_new_tokens is generally easier to control as it only considers the newly generated content\n",
        "\n",
        "# Recommended values:\n",
        "# Short response: 50-100 tokens\n",
        "# Medium response: 100-256 tokens\n",
        "# Long response: 256-512 tokens"
      ],
      "metadata": {
        "id": "6tUDZTIW8vKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rag(\"I just discovered the course. Can I still join it?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pi5ZMQpy0GAl",
        "outputId": "d063e745-e5dc-4bc1-ab9b-a8102ffd4b33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<pad> Yes, even if you don't register, you're still eligible to submit the homeworks. Be aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.</s>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# rag(\"What is the limitation about the course?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JGuL_K9b0Lyl",
        "outputId": "74075757-3019-49ca-a7ed-000a988eae36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<pad> You can follow the course at your own pace after it finishes.</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# rag(\"What is the limitation about the course? What are the requirements?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "LYUMs1MT9cyw",
        "outputId": "790eaf71-b807-466a-f845-b493e9a643ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<pad> You can start by installing and setting up all the dependencies and requirements: Google cloud account Google Cloud SDK Python 3 (installed with Anaconda) Terraform Git Look over the prerequisites and syllabus to see if you are comfortable with these subjects.</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# rag(\"What is the limitation about the course? What are the requirements for our professional background?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EzmbjInR9wX8",
        "outputId": "0abc0bd5-69fe-46bc-813a-1b388ab75fb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<pad> GitHub - DataTalksClub data-engineering-zoomcamp#prerequisites</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rag(\"I just discovered the course. Can I still join it?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "4DFRZIRZ91m2",
        "outputId": "a69a1a14-45d6-475b-92ca-c1a91b1f1cdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Yes, even if you don't register, you're still eligible to submit the homeworks. Be aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    }
  ]
}